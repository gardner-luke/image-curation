{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d777fe7-67c1-40f1-94fd-c91e7f93be00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test CLIP Embedding Models\n",
    "\n",
    "Simple test of the deployed CLIP text and image embedding models using Databricks `ai_query` function.\n",
    "\n",
    "This notebook verifies both embedding endpoints are working correctly by:\n",
    "- Testing text embeddings on sample documentation content\n",
    "- Testing image embeddings on sample crop images from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad14f3c-5224-46b0-aef7-4ee589f27690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Text Embedding Model\n",
    "\n",
    "Test the `clip-text-embedding` endpoint using sample text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0406d6bd-607b-4980-b583-895da23b8d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT content, ai_query('clip-text-embedding', request => substring(content, 1, 77)) AS embedding\n",
    "FROM autobricks.llm_rag_chatbot.databricks_documentation\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9779c8df-5ac4-406f-8303-2b2e88d06606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Image Embedding Model\n",
    "\n",
    "Test the `clip-image-embedding` endpoint using sample crop images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6e9540-729c-479e-8deb-407f83e3c11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT image_base64, ai_query('clip-image-embedding', request => image_base64) AS embedding\n",
    "FROM autobricks.agriculture.crop_images_directory_embeddings\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6db567bf-a79c-46af-9153-f3b294751d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "If both queries above return embeddings without errors, the CLIP models are deployed and working correctly.\n",
    "\n",
    "Next step: Update the full inference pipeline to use these embedding endpoints."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02b - Test CLIP Embedding Model Endpoints",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
