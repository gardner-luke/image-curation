{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9143805-2062-43c3-88d6-9d596b7f4af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch CLIP Embedding for Crop Images with Delta Tables\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Load a registered CLIP model from Unity Catalog for batch processing\n",
    "- Create Spark UDFs to generate embeddings at scale\n",
    "- Process crop image data from Delta tables with CLIP embeddings\n",
    "- Save the enriched data back to Delta tables for downstream ML workflows\n",
    "\n",
    "The resulting table will contain both the original image metadata and high-dimensional CLIP embeddings, enabling semantic search and similarity analysis across the crop image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e57e3a-0dc8-47b4-b056-a88e0f9c9960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries for MLflow model loading, Spark DataFrame processing, and UDF creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d6f0ea-6d1c-4df7-b52d-fe2e6a4f0f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9e1552-4d70-4c05-970e-ea352e7e7775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Configure the dataset-specific parameters. Update these values to match your registered model and table names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160da15f-2596-4e39-8413-bc7a5ead6810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Configuration - Update these values for your dataset\n",
    "CATALOG_NAME = \"autobricks\"  # Your Unity Catalog name\n",
    "SCHEMA_NAME = \"agriculture\"   # Your schema name\n",
    "MODEL_NAME = \"clip_embedding-135\"  # Your registered CLIP model name\n",
    "SOURCE_TABLE_NAME = \"crop_images_directory\"  # Source table from notebook 00\n",
    "OUTPUT_TABLE_NAME = \"crop_images_directory_embeddings\"  # Output table name\n",
    "\n",
    "# Construct full table names\n",
    "FULL_MODEL_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "SOURCE_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{SOURCE_TABLE_NAME}\"\n",
    "OUTPUT_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{OUTPUT_TABLE_NAME}\"\n",
    "\n",
    "# Processing parameters\n",
    "INPUT_COLUMN = \"image_base64\"\n",
    "INPUT_TYPE = \"image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b55e23-d75b-47d0-b60b-7b341b234b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Load Model and Create Embedding UDF\n",
    "\n",
    "Load the registered CLIP model from Unity Catalog and create a Spark UDF for distributed embedding generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9b2b75-c57e-4555-8439-57490a8dbd99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model from Unity Catalog\n",
    "model_uri = f\"models:/{FULL_MODEL_NAME}/1\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "print(f\"Loaded model: {FULL_MODEL_NAME}\")\n",
    "\n",
    "# Create UDF for distributed embedding generation\n",
    "def get_embedding(input_data):\n",
    "    if input_data is None:\n",
    "        return None\n",
    "    \n",
    "    input_df = pd.DataFrame({\"input_data\": [input_data]})\n",
    "    params = {\"input_type\": INPUT_TYPE}\n",
    "    \n",
    "    try:\n",
    "        result = model.predict(input_df, params=params)\n",
    "        return result[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "embedding_udf = udf(get_embedding, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79065ed-983e-4b2d-a1d8-6130b45ee832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Process Images and Generate Embeddings\n",
    "\n",
    "Load the crop images table and apply the CLIP embedding UDF to generate vector representations for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7c1225-a2f5-440d-b3f5-d9dd49bf75c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load source table and apply embedding generation to all rows\n",
    "\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "print(f\"Processing {df.count()} rows from {SOURCE_TABLE}\")\n",
    "\n",
    "# Apply the embedding UDF to the entire DataFrame\n",
    "results_df = df.withColumn(\"embeddings\", embedding_udf(col(INPUT_COLUMN)))\n",
    "\n",
    "# Optionally display a sample of the enriched DataFrame\n",
    "# display(results_df.select(\"file_name\", \"folder\", \"size_bytes\", \"embeddings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee09ad29-0374-4cb0-b4aa-7ed4fbf24439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Save Enriched Data to Delta Table\n",
    "\n",
    "Save the processed DataFrame with embeddings to a new Delta table for downstream ML workflows and semantic search applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a8ce5a-a72e-4dd9-a1be-dce4526e61d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save enriched data with embeddings to Delta table\n",
    "results_df.write \\\n",
    "         .format(\"delta\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(f\"âœ… Enriched data with embeddings saved to {OUTPUT_TABLE}\")\n",
    "print(f\"Table contains {results_df.count()} rows with CLIP embeddings\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "torch",
     "transformers"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02 - Batch CLIP Embedding for Crop Images with Delta Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
