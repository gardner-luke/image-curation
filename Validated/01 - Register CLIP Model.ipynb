{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5bd92c-a2b6-417d-aaa0-0fa18b6700fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple CLIP Model for Databricks Model Serving\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c45f64-5add-40df-9697-b0847a324711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CLIPImageTextEmbedding(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def _get_image_embedding(self, base64_image_str):\n",
    "        # Remove data URL prefix if present\n",
    "        if base64_image_str.startswith('data:image'):\n",
    "            base64_image_str = base64_image_str.split(',')[1]\n",
    "        \n",
    "        decoded_bytes = base64.b64decode(base64_image_str)\n",
    "        image = Image.open(BytesIO(decoded_bytes))\n",
    "        \n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        return image_features.cpu().numpy().tolist()[0]\n",
    "    \n",
    "    def _get_text_embedding(self, text):\n",
    "        inputs = self.processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        \n",
    "        return text_features.cpu().numpy().tolist()[0]\n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        input_type = params.get('input_type')\n",
    "        input_data = model_input['input_data'][0]  # Assuming single string input\n",
    "        \n",
    "        if input_type.lower() == 'image':\n",
    "            return [self._get_image_embedding(input_data)]\n",
    "        elif input_type.lower() == 'text':\n",
    "            return [self._get_text_embedding(input_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7b5157-68df-4ad5-9a29-85e36b5cc807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def register_clip_model(catalog_name=\"main\", schema_name=\"models\", model_name=\"clip_embedding\"):\n",
    "    mlflow.set_experiment(f\"/Shared/clip_embedding_experiment\")\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        # Create model signature\n",
    "        from mlflow.models import infer_signature\n",
    "        \n",
    "        # Sample input for signature inference\n",
    "        sample_input = pd.DataFrame({\"input_data\": [\"sample text\"]})\n",
    "        sample_params = {\"input_type\": \"text\"}\n",
    "        \n",
    "        # Load model to generate sample output\n",
    "        model = CLIPImageTextEmbedding()\n",
    "        model.load_context(None)\n",
    "        sample_output = model.predict(None, sample_input, sample_params)\n",
    "        \n",
    "        signature = infer_signature(sample_input, sample_output, params=sample_params)\n",
    "        \n",
    "        requirements = [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.21.0\",\n",
    "            \"Pillow>=8.3.0\"\n",
    "        ]\n",
    "        \n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"clip_model\",\n",
    "            python_model=model,\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements,\n",
    "            registered_model_name=f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "        )\n",
    "        \n",
    "        return f\"runs:/{run.info.run_id}/clip_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b198a7c-d44f-4bb4-b3ec-c43fbff3c0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_serving_endpoint(model_name, endpoint_name=\"clip-embedding-endpoint\"):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "    \n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    w.serving_endpoints.create(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=\"1\",\n",
    "                    workload_size=\"Small\",\n",
    "                    scale_to_zero_enabled=True,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75ad6ec-b632-4a8d-a2c4-894398b6fdac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    CATALOG_NAME = \"autobricks\"\n",
    "    SCHEMA_NAME = \"agriculture\" \n",
    "    MODEL_NAME = \"clip_embedding-356\"\n",
    "    ENDPOINT_NAME = \"clip-embedding-endpoint-356\"\n",
    "    \n",
    "    # Register model\n",
    "    model_uri = register_clip_model(CATALOG_NAME, SCHEMA_NAME, MODEL_NAME)\n",
    "    \n",
    "    # Create serving endpoint\n",
    "    full_model_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "    create_serving_endpoint(full_model_name, ENDPOINT_NAME)\n",
    "    \n",
    "    print(f\"Model registered as: {full_model_name}\")\n",
    "    print(f\"Serving endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e30550-0b14-4439-8572-17b5c087fa91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\"\"\"\n",
    "# Text embedding\n",
    "payload = {\n",
    "    \"inputs\": {\"input_data\": [\"Hello world\"]},\n",
    "    \"params\": {\"input_type\": \"text\"}\n",
    "}\n",
    "\n",
    "# Image embedding  \n",
    "payload = {\n",
    "    \"inputs\": {\"input_data\": [\"data:image/jpeg;base64,/9j/4AAQ...\"]},\n",
    "    \"params\": {\"input_type\": \"image\"}\n",
    "}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 - Register CLIP Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
