{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5bd92c-a2b6-417d-aaa0-0fa18b6700fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CLIP Model Registration for Databricks Model Serving\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Create a custom MLflow PyFunc wrapper for OpenAI's CLIP model\n",
    "- Handle both image and text embeddings through a unified interface\n",
    "- Register the model to Unity Catalog for production use\n",
    "- Deploy the model to a Databricks Model Serving endpoint\n",
    "\n",
    "The resulting model endpoint can generate embeddings for both images (base64 encoded) and text, enabling semantic similarity searches and multimodal AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries for MLflow model creation, CLIP model handling, and Databricks SDK integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Define CLIP Model Wrapper Class\n",
    "\n",
    "Create a custom MLflow PyFunc class that wraps OpenAI's CLIP model to handle both image and text embeddings through a unified interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c45f64-5add-40df-9697-b0847a324711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CLIPImageTextEmbedding(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def _get_image_embedding(self, base64_image_str):\n",
    "        # Remove data URL prefix if present\n",
    "        if base64_image_str.startswith('data:image'):\n",
    "            base64_image_str = base64_image_str.split(',')[1]\n",
    "        \n",
    "        decoded_bytes = base64.b64decode(base64_image_str)\n",
    "        image = Image.open(BytesIO(decoded_bytes))\n",
    "        \n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        return image_features.cpu().numpy().tolist()[0]\n",
    "    \n",
    "    def _get_text_embedding(self, text):\n",
    "        inputs = self.processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        \n",
    "        return text_features.cpu().numpy().tolist()[0]\n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        input_type = params.get('input_type')\n",
    "        input_data = model_input['input_data'][0]  # Assuming single string input\n",
    "        \n",
    "        if input_type.lower() == 'image':\n",
    "            return [self._get_image_embedding(input_data)]\n",
    "        elif input_type.lower() == 'text':\n",
    "            return [self._get_text_embedding(input_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Define Model Registration Function\n",
    "\n",
    "Create a function to register the CLIP model to Unity Catalog with proper MLflow tracking and dependency management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7b5157-68df-4ad5-9a29-85e36b5cc807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def register_clip_model(catalog_name=\"main\", schema_name=\"models\", model_name=\"clip_embedding\"):\n",
    "    mlflow.set_experiment(f\"/Shared/clip_embedding_experiment\")\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        # Create model signature\n",
    "        from mlflow.models import infer_signature\n",
    "        \n",
    "        # Sample input for signature inference\n",
    "        sample_input = pd.DataFrame({\"input_data\": [\"sample text\"]})\n",
    "        sample_params = {\"input_type\": \"text\"}\n",
    "        \n",
    "        # Load model to generate sample output\n",
    "        model = CLIPImageTextEmbedding()\n",
    "        model.load_context(None)\n",
    "        sample_output = model.predict(None, sample_input, sample_params)\n",
    "        \n",
    "        signature = infer_signature(sample_input, sample_output, params=sample_params)\n",
    "        \n",
    "        requirements = [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.21.0\",\n",
    "            \"Pillow>=8.3.0\"\n",
    "        ]\n",
    "        \n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"clip_model\",\n",
    "            python_model=model,\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements,\n",
    "            registered_model_name=f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "        )\n",
    "        \n",
    "        return f\"runs:/{run.info.run_id}/clip_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Define Serving Endpoint Creation Function\n",
    "\n",
    "Create a function to deploy the registered model to a Databricks Model Serving endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b198a7c-d44f-4bb4-b3ec-c43fbff3c0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_serving_endpoint(model_name, endpoint_name=\"clip-embedding-endpoint\"):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "    \n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    w.serving_endpoints.create(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=\"1\",\n",
    "                    workload_size=\"Small\",\n",
    "                    scale_to_zero_enabled=True,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Configure the model registration parameters. Update these values to match your Unity Catalog setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75ad6ec-b632-4a8d-a2c4-894398b6fdac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'register_clip_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m ENDPOINT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip-embedding-endpoint-356\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Register model to Unity Catalog\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[43mregister_clip_model\u001b[49m(CATALOG_NAME, SCHEMA_NAME, MODEL_NAME)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create serving endpoint\u001b[39;00m\n\u001b[1;32m     11\u001b[0m full_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCATALOG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCHEMA_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'register_clip_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset Configuration - Update these values for your setup\n",
    "CATALOG_NAME = \"autobricks\"  # Your Unity Catalog name\n",
    "SCHEMA_NAME = \"agriculture\"   # Your schema name\n",
    "MODEL_NAME = \"clip_embedding-356\"  # Your model name (make it unique)\n",
    "ENDPOINT_NAME = \"clip-embedding-endpoint-356\"  # Your endpoint name (make it unique)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Execute Model Registration and Deployment\n",
    "\n",
    "Run the model registration and deployment process using the configured parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model to Unity Catalog\n",
    "model_uri = register_clip_model(CATALOG_NAME, SCHEMA_NAME, MODEL_NAME)\n",
    "\n",
    "# Create serving endpoint\n",
    "full_model_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "create_serving_endpoint(full_model_name, ENDPOINT_NAME)\n",
    "\n",
    "print(f\"Model registered as: {full_model_name}\")\n",
    "print(f\"Serving endpoint: {ENDPOINT_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Usage Examples\n",
    "\n",
    "Example payload formats for calling the deployed CLIP model endpoint with both text and image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e30550-0b14-4439-8572-17b5c087fa91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Text embedding example\n",
    "text_payload = {\n",
    "    \"inputs\": {\"input_data\": [\"Hello world\"]},\n",
    "    \"params\": {\"input_type\": \"text\"}\n",
    "}\n",
    "\n",
    "# Image embedding example (base64 encoded image)\n",
    "image_payload = {\n",
    "    \"inputs\": {\"input_data\": [\"data:image/jpeg;base64,/9j/4AAQ...\"]},\n",
    "    \"params\": {\"input_type\": \"image\"}\n",
    "}\n",
    "\n",
    "print(\"Use these payload formats when calling the model serving endpoint:\")\n",
    "print(\"Text embedding:\", text_payload)\n",
    "print(\"Image embedding:\", image_payload)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 - Register CLIP Model",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
