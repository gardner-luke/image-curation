{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7379a7b6-6ffd-4842-9472-ee723c9b0cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CLIP Image Embeddings Batch Processing\n",
    "# This notebook processes the crop_images_directory table to generate CLIP embeddings\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3075b3-06bf-4fab-8b6f-fdfcb0e8814c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the registered CLIP model\n",
    "model_name = \"autobricks.agriculture.clip_embedding-356\"\n",
    "model_version = 1  # or specify a specific alias\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298aab02-0604-4f6e-bece-c9cecfa9a8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define UDF for generating embeddings\n",
    "def generate_clip_embedding(base64_image_str):\n",
    "    \"\"\"\n",
    "    Generate CLIP embedding for a base64 encoded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if base64_image_str is None:\n",
    "            return None\n",
    "        \n",
    "        # Use the loaded model to predict\n",
    "        model_input = {\"input_data\": [base64_image_str]}\n",
    "        params = {\"input_type\": \"image\"}\n",
    "        \n",
    "        embedding = loaded_model.predict(context=None, model_input=model_input, params=params)\n",
    "        return embedding[0]  # Return the embedding list\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Register the UDF\n",
    "embedding_udf = udf(generate_clip_embedding, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e8fc0f-d309-42f3-9dbb-778beb8b8831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the source table\n",
    "source_table = \"autobricks.agriculture.crop_images_directory\"\n",
    "print(f\"Reading source table: {source_table}\")\n",
    "\n",
    "df = spark.table(source_table)\n",
    "\n",
    "# Show schema and sample data\n",
    "print(\"Source table schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f932d5-295c-4c81-992c-eb94fb10284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add CLIP embeddings column\n",
    "print(\"Generating CLIP embeddings...\")\n",
    "df_with_embeddings = df.withColumn(\"CLIP_embedding\", embedding_udf(col(\"image_base64\")))\n",
    "\n",
    "# Show results\n",
    "print(\"Schema with embeddings:\")\n",
    "df_with_embeddings.printSchema()\n",
    "\n",
    "# Count successful embeddings\n",
    "total_records = df_with_embeddings.count()\n",
    "successful_embeddings = df_with_embeddings.filter(col(\"CLIP_embedding\").isNotNull()).count()\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Successful embeddings: {successful_embeddings}\")\n",
    "print(f\"Failed embeddings: {total_records - successful_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9664ef-c736-414f-9107-2a9328445669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_embeddings.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc96061f-fd87-4b11-81f2-b313320f1aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define output table name\n",
    "output_table = \"autobricks.agriculture.crop_images_with_embeddings\"\n",
    "\n",
    "# Write to Delta table\n",
    "print(f\"Saving results to Delta table: {output_table}\")\n",
    "df_with_embeddings.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(output_table)\n",
    "\n",
    "print(f\"✅ Successfully created Delta table: {output_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20bc498-fbac-44ea-afaa-3cd9fb4c4d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the results\n",
    "print(\"\\nVerifying results...\")\n",
    "result_df = spark.table(output_table)\n",
    "result_df.select(\"file_name\", \"folder\", \"size_bytes\", \"CLIP_embedding\").show(5, truncate=False)\n",
    "\n",
    "# Show embedding dimensions\n",
    "sample_embedding = result_df.filter(col(\"CLIP_embedding\").isNotNull()).select(\"CLIP_embedding\").first()\n",
    "if sample_embedding and sample_embedding[\"CLIP_embedding\"]:\n",
    "    embedding_dim = len(sample_embedding[\"CLIP_embedding\"])\n",
    "    print(f\"Embedding dimensions: {embedding_dim}\")\n",
    "\n",
    "print(\"✅ Batch processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-08-26 16_10_17",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
