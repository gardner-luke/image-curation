{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d70131e-b2fb-4d3f-b3f6-06edb1fc4fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Register CLIP Image Embedding Model for Databricks Model Serving\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Create a custom MLflow PyFunc wrapper for CLIP image embeddings\n",
    "- Follow Databricks embedding model conventions for consistent API usage\n",
    "- Register the model to Unity Catalog for production use\n",
    "- Deploy the model to a Databricks Model Serving endpoint\n",
    "\n",
    "The resulting model endpoint can generate image embeddings using standard Databricks API patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945cbd71-fa09-417b-b2b4-0f91dcbe31bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries for MLflow model creation and CLIP image embedding handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4b32ae-2ec5-4a2a-931a-76f56194e4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import base64\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dd20ad-4116-499d-ba98-a4d22b15a7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Configure the model registration parameters for the image embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa9a703-101a-4e1a-96a4-592dbcd5ee5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Configuration - Update these values for your setup\n",
    "CATALOG_NAME = \"autobricks\"  # Your Unity Catalog name\n",
    "SCHEMA_NAME = \"agriculture\"   # Your schema name\n",
    "MODEL_NAME = \"clip_image_embedding\"  # Image embedding model name\n",
    "ENDPOINT_NAME = \"clip-image-embedding\"  # Image embedding endpoint name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7fead2-51b8-4d74-b04d-e58fe0b62109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define CLIP Image Embedding Model Class\n",
    "\n",
    "Create a custom MLflow PyFunc class that wraps CLIP for image embeddings following Databricks conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45edf901-9059-4d5f-8d8d-c7ef70f9a448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CLIPImageEmbedding(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def _process_image(self, image_input):\n",
    "        \"\"\"Process various image input formats to PIL Image.\"\"\"\n",
    "        if isinstance(image_input, str):\n",
    "            # Assume base64 encoded image\n",
    "            try:\n",
    "                decoded_bytes = base64.b64decode(image_input)\n",
    "                image = Image.open(BytesIO(decoded_bytes))\n",
    "            except Exception:\n",
    "                raise ValueError(\"Invalid base64 image string\")\n",
    "        elif isinstance(image_input, bytes):\n",
    "            # Raw bytes\n",
    "            image = Image.open(BytesIO(image_input))\n",
    "        elif hasattr(image_input, 'read'):\n",
    "            # File-like object\n",
    "            image = Image.open(image_input)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image input type: {type(image_input)}\")\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        # Handle different input formats\n",
    "        if isinstance(model_input, str):\n",
    "            # Single base64 image string\n",
    "            images = [model_input]\n",
    "        elif isinstance(model_input, list):\n",
    "            # List of base64 image strings\n",
    "            images = model_input\n",
    "        elif isinstance(model_input, pd.DataFrame):\n",
    "            # DataFrame with image data\n",
    "            if 'input' in model_input.columns:\n",
    "                images = model_input['input'].tolist()\n",
    "            else:\n",
    "                images = model_input.iloc[:, 0].tolist()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input type: {type(model_input)}\")\n",
    "        \n",
    "        embeddings = []\n",
    "        for image_input in images:\n",
    "            # Process image\n",
    "            image = self._process_image(image_input)\n",
    "            \n",
    "            # Generate embedding\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "            \n",
    "            embedding = image_features.cpu().numpy().tolist()[0]\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d805d94-e9c0-4fee-b416-21ebf1f13ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register Image Embedding Model\n",
    "\n",
    "Register the CLIP image embedding model to Unity Catalog with proper MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a88df82-9a93-4e0a-92ba-10f73e9c93ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set experiment\n",
    "mlflow.set_experiment(f\"/Shared/clip_image_embedding_experiment\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Create model instance\n",
    "    model = CLIPImageEmbedding()\n",
    "    \n",
    "    # Create sample base64 image for signature (1x1 white pixel)\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    \n",
    "    # Create a simple test image\n",
    "    test_image = Image.new('RGB', (224, 224), color='white')\n",
    "    buffer = io.BytesIO()\n",
    "    test_image.save(buffer, format='JPEG')\n",
    "    sample_input = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    \n",
    "    # Load model and generate sample output for signature\n",
    "    model.load_context(None)\n",
    "    sample_output = model.predict(None, sample_input)\n",
    "    \n",
    "    # Create signature\n",
    "    from mlflow.models import infer_signature\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "    \n",
    "    # Define requirements\n",
    "    requirements = [\n",
    "        \"torch>=1.9.0\",\n",
    "        \"transformers>=4.21.0\",\n",
    "        \"Pillow>=8.3.0\"\n",
    "    ]\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"clip_image_model\",\n",
    "        python_model=model,\n",
    "        input_example=sample_input,\n",
    "        signature=signature,\n",
    "        pip_requirements=requirements,\n",
    "        registered_model_name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered as: {CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0925928-f2eb-4de8-8995-92c88a2d0e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Serving Endpoint\n",
    "\n",
    "Deploy the registered image embedding model to a Databricks Model Serving endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446f498c-733d-43d3-b59a-147a76e2f4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create serving endpoint\n",
    "full_model_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "\n",
    "w.serving_endpoints.create(\n",
    "    name=ENDPOINT_NAME,\n",
    "    config=EndpointCoreConfigInput(\n",
    "        served_entities=[\n",
    "            ServedEntityInput(\n",
    "                entity_name=full_model_name,\n",
    "                entity_version=\"1\",\n",
    "                workload_size=\"Small\",\n",
    "                scale_to_zero_enabled=True,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Image embedding endpoint created: {ENDPOINT_NAME}\")\n",
    "print(f\"Model: {full_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dd5239-76a9-454b-9314-c73a5133fa6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the Model\n",
    "\n",
    "Test the deployed image embedding model using Databricks standard API patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ad3ea5-8634-46e0-baee-069f987f4bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wait for endpoint to be ready, then test\n",
    "import time\n",
    "\n",
    "print(\"Waiting for endpoint to be ready...\")\n",
    "while True:\n",
    "    try:\n",
    "        endpoint = w.serving_endpoints.get(ENDPOINT_NAME)\n",
    "        if endpoint.state and endpoint.state.ready:\n",
    "            break\n",
    "        time.sleep(30)\n",
    "    except:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"Endpoint ready, testing...\")\n",
    "\n",
    "# Test with standard Databricks embedding API pattern using sample image\n",
    "try:\n",
    "    # Create a test image\n",
    "    test_image = Image.new('RGB', (224, 224), color='blue')\n",
    "    buffer = io.BytesIO()\n",
    "    test_image.save(buffer, format='JPEG')\n",
    "    test_image_b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    \n",
    "    response = w.serving_endpoints.query(\n",
    "        name=ENDPOINT_NAME,\n",
    "        input=test_image_b64\n",
    "    )\n",
    "    print(f\"Image embedding generated successfully: {len(response)} dimensions\")\n",
    "except Exception as e:\n",
    "    print(f\"Test failed: {e}\")\n",
    "\n",
    "print(f\"Image embedding model ready for use: {ENDPOINT_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01b - Register CLIP Image Embedding Model",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
