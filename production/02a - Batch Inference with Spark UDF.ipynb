{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a529f466-6664-4ba8-b68a-85381e5217a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Batch CLIP Embedding for Crop Images with Delta Tables\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Load a registered CLIP model from Unity Catalog for batch processing\n",
    "- Create Spark UDFs to generate embeddings at scale\n",
    "- Process crop image data from Delta tables with CLIP embeddings\n",
    "- Save the enriched data back to Delta tables for downstream ML workflows\n",
    "\n",
    "The resulting table will contain both the original image metadata and high-dimensional CLIP embeddings, enabling semantic search and similarity analysis across the crop image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60794da-e738-4686-906a-ce69d624a1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import only necessary libraries for MLflow model loading, Spark DataFrame processing, and UDF creation\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef2c2de-faf8-49b4-8972-614d15f61ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration: Edit these values for your environment\n",
    "CATALOG_NAME = \"autobricks\"  # Unity Catalog name\n",
    "SCHEMA_NAME = \"agriculture\"   # Schema name\n",
    "MODEL_UC_NAME = \"autobricks.agriculture.clip_image_embedding\"  # Registered CLIP model (Unity Catalog)\n",
    "MODEL_VERSION = 1  # Model version to use\n",
    "SOURCE_TABLE_NAME = \"crop_images_directory\"  # Source table name\n",
    "OUTPUT_TABLE_NAME = \"crop_images_directory_embeddings\"  # Output table name\n",
    "INPUT_COLUMN = \"image_base64\"  # Column containing base64-encoded images\n",
    "INPUT_TYPE = \"image\"  # Input type for the model\n",
    "\n",
    "# Construct full table names\n",
    "SOURCE_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{SOURCE_TABLE_NAME}\"\n",
    "OUTPUT_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{OUTPUT_TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ec27dc-e007-4b6b-ba54-f8b9c7dd5b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 2: Load Model, Define Embedding UDF, and Sample Embedding Generation\n",
    "Continue the streamlined workflow by loading the specified CLIP model, defining the Spark UDF for embedding generation, and providing a sample cell to apply the UDF to 10 rows for verification. All code should reference only the configuration variables and use the correct model. No redundant or legacy code should be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3480bcd4-4432-4bbc-bfa7-3dcc000afbb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the registered CLIP model from Unity Catalog using version number\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{MODEL_UC_NAME}/{MODEL_VERSION}\")\n",
    "\n",
    "def clip_image_embedding_udf(image_base64):\n",
    "    try:\n",
    "        if image_base64 is None:\n",
    "            return None\n",
    "        # Model expects a list of base64 strings\n",
    "        result = model.predict([image_base64])\n",
    "        if result and isinstance(result, list):\n",
    "            return result[0]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Optionally log the error for debugging\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "embedding_udf = udf(clip_image_embedding_udf, ArrayType(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212f46a6-2f9c-4b39-97a8-249300a6a476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample 10 records from the source table for quick validation\n",
    "sample_df = spark.table(SOURCE_TABLE).limit(10)\n",
    "sample_embedded_df = sample_df.withColumn(\"embeddings\", embedding_udf(col(INPUT_COLUMN)))\n",
    "\n",
    "# Display a sample of the output for verification\n",
    "columns_to_show = [INPUT_COLUMN, \"embeddings\"] + [col for col in sample_df.columns if col not in [INPUT_COLUMN, \"embeddings\"]][:3]\n",
    "display(sample_embedded_df.select(*columns_to_show))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca07d00-e40e-4b5c-b8e9-db15352d91e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To process and save the full dataset, uncomment and run the following:\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "results_df = df.withColumn(\"embeddings\", embedding_udf(col(INPUT_COLUMN)))\n",
    "results_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "print(f\"âœ… Enriched data with embeddings saved to {OUTPUT_TABLE}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "torch",
     "transformers"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02a - Batch Inference with Spark UDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
